{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2b583-c1da-4b2e-b4df-2d4ac65e97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install pydicom opencv-python scikit-image\n",
    "# pip install pyradiomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d96161f-6336-49be-ae1a-5f50cbdf5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load\n",
    "import pydicom as dicom\n",
    "import radiomics\n",
    "from radiomics import featureextractor\n",
    "import SimpleITK as sitk\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364a4a5-2187-415e-83f8-3c6c59912354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_images(input_path, output_path, new_width, new_height):\n",
    "    \"\"\"\n",
    "    Cut images into the desired size and save the output images\n",
    "    \n",
    "    Params:  \n",
    "    input_path = path to the original images \n",
    "    output_path = path to save the cut images\n",
    "    new_width = width of the cut images\n",
    "    new_height = height of the cut images\n",
    "    \n",
    "    Return:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create dir if it doesn't exist \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    n_images = 0\n",
    "\n",
    "    # Browse input path\n",
    "    for class_dir in os.listdir(input_path):\n",
    "        class_path = os.path.join(input_path, class_dir)\n",
    "\n",
    "        # If it is a directory \n",
    "        if os.path.isdir(class_path):     \n",
    "\n",
    "            # Save image id\n",
    "            image_id = 1\n",
    "\n",
    "            # Go through images\n",
    "            for image_file in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_file)\n",
    "\n",
    "                # Save patient id\n",
    "                patient = image_file.split(\"_\")[0]                         \n",
    "\n",
    "                image = cv.imread(image_path)\n",
    "\n",
    "                # If image exists\n",
    "                if image is not None:\n",
    "\n",
    "                    # Save subimage id\n",
    "                    sub_id = 1\n",
    "\n",
    "                    for i in range(0, image.shape[0], new_height):\n",
    "                        for j in range(0, image.shape[1], new_width):\n",
    "\n",
    "                            # Cut image into subimage\n",
    "                            sub_image = image[i:i+new_height, j:j+new_width]\n",
    "\n",
    "                            # Output file path\n",
    "                            output_file = f\"{patient}_img{image_id}-{sub_id}.png\"                   \n",
    "                            output_file = os.path.join(output_path, class_dir, output_file)                        \n",
    "\n",
    "                            # Save subimage\n",
    "                            cv.imwrite(output_file, sub_image)                 \n",
    "\n",
    "                            sub_id += 1\n",
    "                            n_images += 1\n",
    "\n",
    "                    image_id += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54a4d795-c9a9-49b4-8f07-2f43233fa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(input_path):\n",
    "    \"\"\"\n",
    "    Read images in the input_path, \n",
    "    save image, patient of each image and the class (group/labels)\n",
    "    \n",
    "    Params: \n",
    "    input_path = path to the original images \n",
    "    \n",
    "    Return:\n",
    "    images = list of all images\n",
    "    patients = list with patient id for each image\n",
    "    classes = list with class for each image\n",
    "    \"\"\"\n",
    "   \n",
    "    # Lists to save images, patients and classes\n",
    "    images = []\n",
    "    patients = []\n",
    "    classes = []\n",
    "\n",
    "    # Browse input path\n",
    "    for class_dir in os.listdir(input_path):\n",
    "        class_path = os.path.join(input_path, class_dir)\n",
    "\n",
    "        # If it is a directory \n",
    "        if os.path.isdir(class_path):    \n",
    "\n",
    "            for image_file in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_file)\n",
    "\n",
    "                patient = image_file.split(\"_\")[0]             \n",
    "\n",
    "                image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "                # Append image, patient id and class to list\n",
    "                images.append(image)\n",
    "                patients.append(patient)\n",
    "                classes.append(class_dir)    \n",
    "                \n",
    "    return (images, patients, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b932850-3b26-48ae-b7fd-84bc4b01f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_folds(images, patients, classes):\n",
    "    \"\"\"\n",
    "    Divides a dataset into folds for stratified k-fold cross-validation.\n",
    "    \n",
    "    Params: \n",
    "    images = list of all images\n",
    "    patients = list with patient id for each image\n",
    "    classes = list with class for each image\n",
    "    \n",
    "    Return:\n",
    "    folds = list of tuples, each tuple is one folder\n",
    "    \"\"\"\n",
    "    # Create a list of unique indexes for patients\n",
    "    unique_patients = list(set(patients))\n",
    "\n",
    "    # Shuffle the list of unique indexes\n",
    "    random.shuffle(unique_patients)\n",
    "\n",
    "    # Divide patients into groups\n",
    "    n_folds = 4 # since it's not an exact division, there will be 5 folds\n",
    "    fold_size = len(unique_patients) // n_folds\n",
    "    patients_folds = [unique_patients[i:i+fold_size] for i in range(0, len(unique_patients), fold_size)]\n",
    "\n",
    "    # List to save folds\n",
    "    folds = []\n",
    "\n",
    "    # Divide images into folds based on patients\n",
    "    for i, patients_folds in enumerate(patients_folds):\n",
    "        train_patients = [p for p in unique_patients if p not in patients_folds]\n",
    "        test_patients = patients_folds\n",
    "\n",
    "        train_indices = [i for i, patient in enumerate(patients) if patient in train_patients]\n",
    "        test_indices = [i for i, patient in enumerate(patients) if patient in test_patients]\n",
    "\n",
    "        train_images = [images[i] for i in train_indices]\n",
    "        test_images = [images[i] for i in test_indices]\n",
    "        train_classes = [classes[i] for i in train_indices]\n",
    "        test_classes = [classes[i] for i in test_indices]\n",
    "\n",
    "        folds.append((train_images, test_images, train_classes, test_classes))\n",
    "        \n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ffde47d-d3b1-4b23-aa4a-624176aa0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholds(imgs):    \n",
    "    \"\"\"\n",
    "    Apply Otsu's and Adaptative thresholds to images\n",
    "    \n",
    "    Params: \n",
    "    imgs = list of raw images\n",
    "    \n",
    "    Return: \n",
    "    imgs_otsu = Otsu's thresholded images\n",
    "    imgs_adapt = Adaptative thresholded images    \n",
    "    \"\"\"\n",
    "    \n",
    "    imgs_otsu = []\n",
    "    imgs_adapt = []\n",
    "    \n",
    "    # For each image in dataset                          \n",
    "    for img in imgs: \n",
    "        \n",
    "        # Otsu's thresholding\n",
    "        _, th1 = cv.threshold(img, 100, 1, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "        \n",
    "        # Adaptative gaussian thresholding        \n",
    "        #th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY,11,2)\n",
    "        th3 = cv.adaptiveThreshold(img, 1, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 11, 2)\n",
    "               \n",
    "        imgs_otsu.append(th1)\n",
    "        imgs_adapt.append(th3)    \n",
    "        \n",
    "    return (imgs_otsu, imgs_adapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca7cfa20-c693-47ef-acfc-3a17fa27ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_folds(folds):\n",
    "    \"\"\"\n",
    "    For each fold, add Otsu's and adaptive\n",
    "    thresholding of all test and train images\n",
    "    \n",
    "    Params:\n",
    "    folds = list of tuples containing the images divided in folds\n",
    "    \n",
    "    Returns:\n",
    "    new_folds = updated list of tuples containing the images and masks divided in folds\n",
    "    \"\"\"\n",
    "    \n",
    "    new_folds = []\n",
    "    \n",
    "    # For each fold\n",
    "    for i in range(len(folds)):\n",
    "        fold = folds[i]        \n",
    "\n",
    "        # Get train and test images\n",
    "        x_train = fold[0]\n",
    "        x_test = fold[1]\n",
    "        y_train = fold[2]\n",
    "        y_test = fold[3]\n",
    "\n",
    "        x_train_otsu, x_train_adapt = apply_thresholds(x_train)\n",
    "\n",
    "        # Update list with the thresholds   \n",
    "        new_folds.append((x_train, x_train_otsu, x_train_adapt, x_test, y_train, y_test))\n",
    "        \n",
    "    return new_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e1e017d-32d8-466e-8cb3-35b6a0ca267d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../folds.joblib']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cut images into 40x30\n",
    "#cut_images(\"imagens_ihq_er\", \"imagens_cortadas\", 40, 30)\n",
    "\n",
    "# Get images, patients and classes from a path\n",
    "images, patients, classes = read_images(\"imagens_cortadas\")\n",
    "\n",
    "# Divide images into folds\n",
    "folds = divide_folds(images, patients, classes)\n",
    "\n",
    "# Update folds, applying Otsu's and adaptive thresholds\n",
    "folds = update_folds(folds)\n",
    "\n",
    "# Save folds locally\n",
    "dump(folds, '../folds.joblib')\n",
    "\n",
    "# Load folds\n",
    "# folds = load('../folds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67be011-d4cc-4466-9ee2-db590c153ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load folds\n",
    "folds = load('../folds.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f16db1-548d-4e29-aadc-a7d72c4493ec",
   "metadata": {},
   "source": [
    "Extract features with PyRadiomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df52a57-9ed6-47f8-98ab-ad4a6c4c7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extractor(imgs, otsu, adapt, extractor):    \n",
    "    \"\"\"\n",
    "    Extract features using sitk and pyradiomics\n",
    "    \n",
    "    Params:\n",
    "    imgs = raw images\n",
    "    otsu = masked images with otsu thresholding\n",
    "    adapt = masked images with adaptative thresholding\n",
    "    extractor = pyradiomics extractor\n",
    "    \n",
    "    Returns:\n",
    "    features_otsu = features for otsu mask\n",
    "    features_adapt = features for adaptative mask\n",
    "    \"\"\"\n",
    "    \n",
    "    data_spacing=[1,1,1]\n",
    "    features_otsu = []\n",
    "    features_adapt = []    \n",
    "       \n",
    "    for idx in range(len(imgs)): \n",
    "\n",
    "        # Get raw, Otsu's and adaptive images\n",
    "        img = imgs[idx]\n",
    "        img_otsu = otsu[idx]\n",
    "        img_adapt = adapt[idx]\n",
    "\n",
    "        sitk_img = sitk.GetImageFromArray(img)\n",
    "        sitk_img.SetSpacing((1, 1, 1))\n",
    "        sitk_img = sitk.JoinSeries(sitk_img)\n",
    "\n",
    "        sitk_otsu = sitk.GetImageFromArray(img_otsu)\n",
    "        sitk_otsu.SetSpacing((1, 1, 1))\n",
    "        sitk_otsu = sitk.JoinSeries(sitk_otsu)\n",
    "        sitk_otsu = sitk.Cast(sitk_otsu, sitk.sitkInt32)\n",
    "\n",
    "        sitk_adapt = sitk.GetImageFromArray(img_adapt)\n",
    "        sitk_adapt.SetSpacing((1, 1, 1))\n",
    "        sitk_adapt = sitk.JoinSeries(sitk_adapt)\n",
    "        sitk_adapt = sitk.Cast(sitk_otsu, sitk.sitkInt32)       \n",
    "        \n",
    "        # Extract features and append them to the proper list\n",
    "        try:\n",
    "            ft_otsu = extractor.execute(sitk_img, sitk_otsu)\n",
    "            features_otsu.append(ft_otsu)\n",
    "            \n",
    "            ft_adapt = extractor.execute(sitk_img, sitk_adapt)\n",
    "            features_adapt.append(ft_adapt)   \n",
    "                \n",
    "        except: \n",
    "            #print(f\"{idx}, \", end=\"\")\n",
    "            pass          \n",
    "        \n",
    "    return (features_otsu, features_adapt)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "283f005e-92be-49b2-b869-f622389d0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_append(element, dest):\n",
    "    \"\"\"\n",
    "    Append element to the list destiny, if element is not in destiny\n",
    "    \n",
    "    Params:\n",
    "    element = an element of any kind\n",
    "    dest = a destination list\n",
    "    \n",
    "    Returns:\n",
    "    destiny = list with appended element if the element was not in there\n",
    "    \"\"\"    \n",
    "    if element not in dest:\n",
    "        dest.append(element)\n",
    "        \n",
    "    return dest\n",
    "\n",
    "def process_features(feats_o, feats_a):\n",
    "    \"\"\"\n",
    "    Process features, in a way that:\n",
    "    - features that are dictionaries and strings are removed\n",
    "    - features that are tuples are separated and each element \n",
    "    of the tuple is considered one feature\n",
    "    - other types are converted to float\n",
    "    \n",
    "    Params:\n",
    "    feats_o = list of Otsu's threshold features\n",
    "    feats_a = list of adaptativa threshold features\n",
    "    \n",
    "    Returns:\n",
    "    all_feats_o = Otsu's features processed\n",
    "    all_feats_a = adaptative features processed\n",
    "    names = feature names processed\n",
    "    \"\"\"    \n",
    "    \n",
    "    all_feats_o = []\n",
    "    all_feats_a = []\n",
    "    names = [] \n",
    "    \n",
    "    # For each image in one of the features list\n",
    "    for sample_id in range(len(feats_o)):\n",
    "\n",
    "        # Get features for Otsu's and adaptive for this sample\n",
    "        sample_o = feats_o[sample_id]\n",
    "        sample_a = feats_a[sample_id]\n",
    "\n",
    "        values_o = []\n",
    "        values_a = []   \n",
    "\n",
    "        # For each feature in the list\n",
    "        for key in sample_o:\n",
    "\n",
    "            # Get the feature's value\n",
    "            value_o = sample_o[key]\n",
    "            value_a = sample_a[key]\n",
    "\n",
    "            # If the value is str or dict, ignore it\n",
    "            if type(value_o) == str or type(value_o) == dict:\n",
    "                continue\n",
    "            # If it's a tuple\n",
    "            elif type(value_o) == tuple:        \n",
    "                for e in range(len(value_o)):   \n",
    "                    # Add and index to the feature name\n",
    "                    conditional_append(f'{key}_{e}', names)\n",
    "                    # Append float values to the lists\n",
    "                    values_o.append(float(value_o[e]))\n",
    "                    values_a.append(float(value_a[e]))\n",
    "            # For other data types, just append the name and float values\n",
    "            else:\n",
    "                conditional_append(key, names)\n",
    "                values_o.append(float(value_o))\n",
    "                values_a.append(float(value_a))     \n",
    "    \n",
    "        # Append processed features to the general list\n",
    "        all_feats_o.append(values_o)\n",
    "        all_feats_a.append(values_a) \n",
    "    \n",
    "    return (all_feats_o, all_feats_a, names)\n",
    "\n",
    "def extract_features(folds):   \n",
    "    \"\"\"\n",
    "    Process features, in a way that:\n",
    "    - features that are dictionaries and strings are removed\n",
    "    - features that are tuples are separated and each element \n",
    "    of the tuple is considered one feature\n",
    "    - other types are converted to float\n",
    "    Get the features' names, with tuple features indexed \n",
    "    \n",
    "    Params:\n",
    "    folds = list of tuples containing x_train raw and with thresholds\n",
    "    \n",
    "    Returns:\n",
    "    all_folds_feats = dictionary containing Otsu's features and adaptive\n",
    "    features for each fold\n",
    "    names = feature names\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Create feature extractor\n",
    "    # !wget -c https://raw.githubusercontent.com/AIM-Harvard/pyradiomics/master/examples/exampleSettings/Params.yaml\n",
    "    params = 'Params.yaml'\n",
    "    settings = {'label': 1, 'correctMask': True}\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor(params, additionalInfo=True, **settings)\n",
    "\n",
    "    # Dictionary to keep all features for all folds\n",
    "    all_folds_feats = {}\n",
    "    \n",
    "    # For each fold\n",
    "    for fold_id in range(len(folds)):\n",
    "\n",
    "        fold = folds[fold_id]\n",
    "        \n",
    "        # Get x_train and threshold images\n",
    "        x_train = fold[0]\n",
    "        x_train_otsu = fold[1]\n",
    "        x_train_adapt = fold[2]\n",
    "\n",
    "        # Extract features from Otsu's and adaptative\n",
    "        feats_o, feats_a = run_extractor(x_train, x_train_otsu, x_train_adapt, extractor)\n",
    "\n",
    "        # Process features and get feature names\n",
    "        all_feats_o, all_feats_a, names = process_features(feats_o, feats_a)        \n",
    "            \n",
    "    all_folds_feats[fold_id+1] = (all_feats_o, all_feats_a)   \n",
    "\n",
    "    return (all_folds_feats, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c20992f0-183f-4cff-b718-67608f9100ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folds_feats, names = extract_features(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a68bb17b-0b53-4633-b093-f0073d8aac95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_folds_feats[1][0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
